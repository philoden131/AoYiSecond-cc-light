---
para: area
domain: 领域
type: 笔记
tags: [调研, 洞察]
status: active
summary: 移动互联网更喜欢艺术家 边际成本很低 可以情怀博一个同频共振的用户群
created: 2026-02-13
updated: 2026-02-13
reviewed_at: 2026-02-14
---

移动互联网更喜欢艺术家 边际成本很低 可以情怀博一个同频共振的用户群

AI创业更像传统制造业，会有固定的成本存在（token费用）

品味体现在内部的 evaluation 或者 内部的benchmark 上

GPA

Agent 和 新一代模型一起发布 去享受新模型的红利 直接一个代际碾压 产生Aha时刻

AI时代 会把创业公司的产能放的很大 因此 不做什么 是很重要的

以前可能没有这个奢侈 不存在“思考不做什么”的生产力

为什么通用 不垂直？ 从从产品层面上来说，做通用会让用户打开的频次变高。做垂直天然的，它就会在某一个场景下才会打开。其次， 在Agent的这个视角下 它的底层架构是一样的，用的是通用的沙河环境和通用的大模型。所以做垂直只是在通用里面去做限制，他们只是现在没有做这个限制。

引申思考：
如果完成的任务是需要多步骤的，那它就应该异步 不合用户抢 2.如果完成的任务是少步骤的，那还不如用户自己点选来得快 --》那是不是可以在对话框中给出UI让用户点，或者给出UI组件（添加完筛选项的）
=== 比用户能做的事情 多做一步 所有垂直场景 在多加一步 给用户的交付感更强（帮你写一篇邮件，写完还能直接帮你发出去）

=== 模型对于还有多少个token能用，其实是有感知的 所以对于长尾的输出呃，长token量的输出模型是知道的这也是一个很重要的优化点。

=== 从上下文可能没那么重要，重要的是让模型能有意识的去压缩自己的上下文和offload自己的上下文。并且能感知什么时候可以用到这个上下文去哪里拿拿来了继续用。这个能力很重要； 让模型意识到上下文窗口中有东西被压缩了这件事情是需要单独训练的。目前的大模型放出来的都是针对于chat bot的能力去走后训练的。

==== 人的分工可能不适配于模型agent之间的分工。因为人的分工是适合于人的人的大脑的带宽、智力、知识面其实都没有agent或者大模型那么全面。所以大模型可以有分工，但是不应该颗粒度按照人的这一套关系思维去做。它肯定有一个它的边界，比如说设置于模态，设置于上下文，设置于注意力，但它的那个边界跟人的边界肯定是不对齐的。因为人跟agent和大模型之间肯定还有很大的差距。所以用人的分工方式路径依赖去限制agent或者说多agent系统一定是会存在问题的。

==== 通用Aent和垂直Aent是在于输入和输出不同。通用更依赖于人去下达任务。但垂直ent它的那个输入端一定是介入到某一个现有的系统里面，它可以自动去获取的。相当于有一很长段上下文是在系统里面无感的去收取的。然后人只要输入。呃对应垂直领域下的简单内容就行了。对于这一段，其实我感触很深，因为做的调整和穿搭都是这样的，就是我们让人输入的上下文，我们就经默获取，可以获得任为镜的做更好的任务完成。然后确实我在思考agent的架构设计的时候，其实跟现有的整一个大类的agent的框架都是通用的。

做垂直ToC的 比如我要做一个剪辑Agent，符合这个剪辑Agent 肯定不是去替代剪辑师的，也不是去给剪辑师用的，而是去给有剪辑需求的普通人用的这个切入点是最好的。因为如果要做很专业的话，对于专业的人来说，他是很懂这个事情的。一旦你不好用，它就是一个不断扣分的过程。那对于普通人来说，只要你有一点好用，它就是一个加分的过程。因为他不专业，所以他不知道原本专业的，应该是什么样的。

（专业的人是从风险控制的角度去看的，因为他有专业知识 / ToC的垂类Agent应该做，给原本有需求，但是做不了的人，这种垂直Agent对这类人来说是净增益）

=== 如果从替换人的心智去做，大家都会去风险控制 从增强人的心智去做，大家都会去感受增量 人还是在流程中，只是把人的生产力拉升很多

==== 系统三要素：模型+环境+用户

=== 成功之后就会被过渡解读 第一个通用Agent 的定义 其实是 因为拍视频的时候被保洁阿姨挡住镜头 而不得已为之的

对不同领域的客户宣传的口径和讲解的口径肯定是不一样的。虽然都是垂直，但是我们可以找一个针对性的点去讲，还是站在用户的立场告诉用户。

季逸超在这里对话中提到了很多次，manus像人一样怎么去做。但是在设计manus的时候，不能把它类比成人。因为他对外的时候，其实是用户的感受，就把它类比成一个打工人的形象。但是在设计的时候，要根据模型的特质去设置，而不能类比成人的特质去设置，这样会导致系统的合理性受到偏差。对外他要表现的这一个人，但是在内部设计的时候，我们不能把模型当成人去看。

把Aent人格化是人类一种自恋。

强如manus，强如季逸超、肖红他们团队，他们的定价也是拍脑袋决定的。 因为agent的消耗是没办法去准确估量的。他的不同任务消耗的量是肯定不一样的。而且他们的定价只是找了GPT的一个锚点去做的。

=== AI时代的创业或者agent的创业。其实DAU不是核心指标，核心指标是为用户创造了多少价值。那再往后推，最直观的就是消耗了多少头ken，再往后推就是公司赚了多少钱。如果是按token计费的话。

=== 网络效应有两种 1 - 基于其他人的产出进一步的做贡献（build on someone else's work） 2- 人和人之间的社交网络，用户之间的关系

AI时代没有看到网络效应 因为AI在两者上是附加值，去提效的，而不是去产生这两种关系的

===

SandboxTeam ：教不会用电脑的人去用电脑，去做一个适配LLM特性的运行环境

AgentTeam：设计一个稳健但统一的架构让我们能够持续的跟上模型的进步 （保证每一次模型的迭代，我们的框架能够收益最多） 方法论：弱到强的衡量—— Agent中有两个变量会影响质量 【模型】和 【Agent框架】 消融对比： 先锁死模型框架，然后用同源的模型家族的弱版本和强版本对比（比如 Claude 3.5 和 Claude4.5 ） 跑同样的benchmark，然后不断的调整Agent框架去做优化， 优化目标是 他们之间的Delta(差距）最大 这样是为了下一代模型最强的时候，我获得的增幅是最大的

manus每次新增单点能力都不只是为了单点，而是会让整个系统都升级，单点的原子能力会和其他已有的原子能力产生化学反应的 复杂系统 而不是 头痛医头，用系统全局的眼光去迭代的

每次有一个新需求更新一个单点的功能。嗯，其实在当下会很快的解决那个新需求。但是从长远来看，呃，功能点多了之后要维护起来，其实是很复杂很重的事情。

==== 我们没有盲目的去追Reasoning这条路 其实你按现在比较朴素意义上的Reasoning Model,你会导致Instruction Following跟这个 Tool Call Hallucinations的增加

所以这个是我们当时做的比较重要的一点，我们用了一个比较另类的方法，

就有一个单独的planning stage来做的这个事。

==== 推理的训练应该 结合 可用工具一起去推理，而不是只做纯理论的推理

====

AI接下来的进步需要用户的参与