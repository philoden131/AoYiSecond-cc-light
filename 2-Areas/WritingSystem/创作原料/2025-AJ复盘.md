---
para: area
domain: 领域
type: 笔记
tags: [调研, 洞察]
status: active
summary: 2025 年是大语言模型（LLM）突飞猛进的一年。下面我整理了一份个人视角的「范式转变」清单——那些让我眼前一亮、改变了行业格局的事件和洞见。
created: 2026-02-13
updated: 2026-02-13
reviewed_at: 2026-02-14
---

# 2025 大语言模型年度复盘

2025 年是大语言模型（LLM）突飞猛进的一年。下面我整理了一份个人视角的「范式转变」清单——那些让我眼前一亮、改变了行业格局的事件和洞见。

## 1. 可验证奖励的强化学习（RLVR）

2025 年初，各大实验室训练 LLM 的标准流程大概是这样的：

1. **预训练**（GPT-2/3 时代，约 2020 年）
2. **监督微调**（InstructGPT，约 2022 年）
3. **人类反馈强化学习 RLHF**（约 2022 年）

这套配方经过验证，一直是训练生产级 LLM 的主流方案。但 2025 年，**可验证奖励的强化学习（RLVR）** 作为新的关键阶段闪亮登场。

它的原理是：让 LLM 在一些可自动验证结果的任务中训练（比如数学题、代码题），模型会自发地学会「推理」——学会把复杂问题拆解成中间步骤，学会各种解题策略和纠错技巧（具体案例可参考 DeepSeek R1 论文）。这些策略用传统方法很难实现，因为我们根本不知道对 LLM 来说「最优推理路径」长什么样——只能让它自己在奖励函数的指引下摸索出来。

跟监督微调和 RLHF 不同（这两个阶段计算量相对较小），RLVR 使用的是客观的、不可钻空子的奖励函数，因此可以跑很长时间的优化。实践证明 RLVR 的「能力/成本比」极高，吃掉了原本用于预训练的算力。所以 2025 年的能力提升，主要来自各家实验室疯狂消化这个新阶段的红利——模型参数量差不多，但强化学习跑得更久了。

RLVR 还带来了一个全新的「调节旋钮」：**推理时计算**（test-time compute）——通过生成更长的推理链条、给模型更多「思考时间」，就能继续提升能力。OpenAI 的 o1（2024 年底）是第一个 RLVR 模型的演示，而 o3（2025 年初）才是真正让人「直观感受到差距」的拐点。

## 2. 幽灵 vs. 动物 / 参差不齐的智能

2025 年是我（相信行业里很多人也是）第一次真正「理解」LLM 智能的形状。

我们不是在「培育动物」，而是在「召唤幽灵」。

LLM 的一切都不一样——神经网络架构、训练数据、训练算法，尤其是优化目标。既然如此，我们得到的当然是智能空间里非常不同的存在，根本不适合用「动物」的视角去理解。

从信息量的角度看：人类神经网络被优化的目标是「让部落在丛林中生存」，而 LLM 神经网络被优化的目标是「模仿人类的文本、在数学题里得分、在 LM Arena 上获得人类的点赞」。

由于 RLVR 只在可验证领域有效，LLM 的能力会在这些领域「突刺」，整体呈现出一种滑稽的「参差不齐」——它同时是博学的天才和糊涂的小学生，上一秒还在解高深的数学题，下一秒就被一个越狱提示骗得把你的数据全吐出来。

> 人类智能：蓝色；AI 智能：红色。这张梗图（抱歉找不到原出处了）的精妙之处在于：它也点明了人类智能同样是参差不齐的，只是方式不同。

说到这儿，就不得不提我 2025 年对 benchmark 的态度：**冷淡，失去信任**。

根本问题在于：benchmark 几乎天然就是可验证环境，因此立刻会被 RLVR（或其弱化版本——通过合成数据生成）钻空子。在典型的「刷榜」过程中，实验室团队不可避免地会在 embedding 空间里找到 benchmark 附近的小区域，然后长出「尖刺」去覆盖它们。「在测试集上训练」已经成了一门新艺术。

> 刷爆所有 benchmark 却依然得不到 AGI，那会是什么样子？

关于这部分，我写过更多内容，可参考：
- Animals vs. Ghosts
- Verifiability
- The Space of Minds

## 3. Cursor / LLM 应用的新层级

Cursor 今年异军突起，但最让我印象深刻的是：它清晰地揭示了「LLM 应用」这个新层级的存在——人们开始说「某某领域的 Cursor」。

正如我在今年 Y Combinator 演讲中强调的，像 Cursor 这样的 LLM 应用会为特定垂直领域打包和编排 LLM 调用：

- 它们做「上下文工程」
- 它们在底层编排多个 LLM 调用，串成越来越复杂的 DAG，在性能和成本之间精心平衡
- 它们为「人在回路」提供专用的 GUI
- 它们提供一个「自主程度滑块」

2025 年有很多讨论：这个新的应用层有多「厚」？LLM 实验室会通吃所有应用，还是 LLM 应用仍有广阔天地？

我个人的判断：LLM 实验室的趋势是培养出「能力全面的大学毕业生」，而 LLM 应用会把这些毕业生组织起来、微调训练，注入私有数据、传感器、执行器和反馈回路，最终在垂直领域「激活」成专业人士。

## 4. Claude Code / 住在你电脑里的 AI

Claude Code（CC）是 2025 年第一个让人信服的「LLM Agent」演示——它用一种循环的方式把工具调用和推理串起来，完成复杂的问题解决。

更重要的是：CC 跑在你的电脑上，使用你的私有环境、数据和上下文。

我认为 OpenAI 在这件事上走了弯路——他们把 Codex / Agent 的精力放在了云端容器部署、从 ChatGPT 编排，而不是 `localhost`。

虽然「云端 Agent 蜂群」听起来像「AGI 终局」，但我们生活在一个能力参差不齐、起飞速度适中的过渡世界。在这个阶段，把 Agent 直接跑在电脑上、与开发者的具体环境手拉手协作，其实更合理。

CC 抓住了这个优先级，并用一个极简、优雅、令人心动的 CLI 形态呈现出来，改变了「AI 是什么样子」——它不再只是一个你去访问的网站（像 Google），而是一个「住在你电脑里」的小精灵/幽灵。

这是一种全新的、独特的 AI 交互范式。

## 5. Vibe Coding（氛围编程）

2025 年，AI 的能力跨过了一个临界点：你可以纯粹用英语描述需求，让 AI 帮你写出各种令人惊艳的程序——完全不用关心代码本身。

有意思的是，「vibe coding」这个词是我在一条灵光乍现的推文里随口造的，完全没想到会火成这样 :)

有了 vibe coding，编程不再专属于受过高度训练的专业人员——任何人都可以做到。这正是我在《Power to the People: How LLMs flip the script on technology diffusion》中写过的：与以往所有技术截然不同，LLM 让普通人比专业人士、公司和政府受益更多。

但 vibe coding 不仅赋能普通人，也解放了专业开发者——我们可以写出更多「本来不会写」的软件。

我用 vibe coding 在 nanochat 项目里用 Rust 写了一个高效的 BPE tokenizer，根本不需要依赖现有库，也不用深入学 Rust。我今年用 vibe coding 快速搭建了很多小项目 demo（比如 menugen、llm-council、reader3、HN time capsule）。我甚至 vibe coded 过一些「一次性」的临时应用，只是为了定位某个 bug——反正代码突然变得免费、临时、可塑、用完即扔了。

Vibe coding 将重塑软件行业，改写岗位描述。

## 6. Nano Banana / LLM 的 GUI

Google Gemini Nano Banana 是 2025 年最不可思议、最具范式颠覆性的模型之一。

在我看来，LLM 是继 1970-80 年代计算机之后的下一个重大计算范式。因此，我们会看到类似的创新——出于本质上相似的原因。我们会看到「个人计算」的等价物、「微控制器」（认知核心）的等价物、「互联网」（Agent 互联网）的等价物等等。

具体到交互体验：跟 LLM「聊天」有点像 1980 年代在电脑控制台敲命令。文本是计算机（和 LLM）偏好的原始数据格式，但不是人类偏好的格式——尤其在输入端。人们其实不喜欢阅读文字，太慢太费力。人们喜欢视觉化、空间化地消费信息，这正是传统计算中 GUI 被发明出来的原因。

同理，LLM 也应该用人类偏好的格式跟我们说话——图片、信息图、幻灯片、白板、动画/视频、Web 应用……

目前的早期版本当然是 emoji 和 Markdown——通过标题、加粗、斜体、列表、表格等方式「装扮」和布局文本，让它更易于视觉消费。

但谁来真正构建「LLM 的 GUI」呢？

在这个视角下，Nano Banana 是一个早期的信号。重要的是：它不仅仅是图像生成，而是**文本生成、图像生成和世界知识三者的联合能力**，全部编织在模型权重里。

---

## 总结

2025 年是 LLM 令人兴奋、略带惊喜的一年。

LLM 正在涌现为一种新型智能——比我预期的聪明得多，同时也比我预期的蠢得多。无论如何，它们极其有用。我认为行业目前连它们潜力的 10% 都还没释放出来。

与此同时，还有太多想法值得尝试，整个领域在概念层面感觉一片开阔。

正如我今年早些时候在 Dwarkesh 播客上说的：我同时（看似矛盾地）相信，我们既会看到快速且持续的进步，*同时*还有大量工作要做。

系好安全带。

---

*原文链接：本文已同步发布在我的博客上，排版和阅读体验会更好一些。*
